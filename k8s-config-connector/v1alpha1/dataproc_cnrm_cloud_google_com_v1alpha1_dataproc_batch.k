"""
This file was generated by the KCL auto-gen tool. DO NOT EDIT.
Editing this file might prove futile when you re-run the KCL auto-gen generate command.
"""
import k8s.apimachinery.pkg.apis.meta.v1


schema DataprocBatch:
    r"""
    DataprocBatch is the Schema for the DataprocBatch API

    Attributes
    ----------
    apiVersion : str, default is "dataproc.cnrm.cloud.google.com/v1alpha1", required
        APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources
    kind : str, default is "DataprocBatch", required
        Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds
    metadata : v1.ObjectMeta, default is Undefined, optional
        metadata
    spec : DataprocCnrmCloudGoogleComV1alpha1DataprocBatchSpec, default is Undefined, required
        spec
    status : DataprocCnrmCloudGoogleComV1alpha1DataprocBatchStatus, default is Undefined, optional
        status
    """


    apiVersion: "dataproc.cnrm.cloud.google.com/v1alpha1" = "dataproc.cnrm.cloud.google.com/v1alpha1"

    kind: "DataprocBatch" = "DataprocBatch"

    metadata?: v1.ObjectMeta

    spec: DataprocCnrmCloudGoogleComV1alpha1DataprocBatchSpec

    status?: DataprocCnrmCloudGoogleComV1alpha1DataprocBatchStatus


schema DataprocCnrmCloudGoogleComV1alpha1DataprocBatchSpec:
    r"""
    DataprocBatchSpec defines the desired state of DataprocBatch

    Attributes
    ----------
    environmentConfig : DataprocCnrmCloudGoogleComV1alpha1DataprocBatchSpecEnvironmentConfig, default is Undefined, optional
        environment config
    labels : {str:str}, default is Undefined, optional
        Optional. The labels to associate with this batch. Label **keys** must contain 1 to 63 characters, and must conform to [RFC 1035](https://www.ietf.org/rfc/rfc1035.txt). Label **values** may be empty, but, if present, must contain 1 to 63 characters, and must conform to [RFC 1035](https://www.ietf.org/rfc/rfc1035.txt). No more than 32 labels can be associated with a batch.
    location : str, default is Undefined, optional
        Required.
    projectRef : DataprocCnrmCloudGoogleComV1alpha1DataprocBatchSpecProjectRef, default is Undefined, optional
        project ref
    pysparkBatch : DataprocCnrmCloudGoogleComV1alpha1DataprocBatchSpecPysparkBatch, default is Undefined, optional
        pyspark batch
    resourceID : str, default is Undefined, optional
        The DataprocBatch name. If not given, the metadata.name will be used.
    runtimeConfig : DataprocCnrmCloudGoogleComV1alpha1DataprocBatchSpecRuntimeConfig, default is Undefined, optional
        runtime config
    sparkBatch : DataprocCnrmCloudGoogleComV1alpha1DataprocBatchSpecSparkBatch, default is Undefined, optional
        spark batch
    sparkRBatch : DataprocCnrmCloudGoogleComV1alpha1DataprocBatchSpecSparkRBatch, default is Undefined, optional
        spark r batch
    sparkSQLBatch : DataprocCnrmCloudGoogleComV1alpha1DataprocBatchSpecSparkSQLBatch, default is Undefined, optional
        spark SQL batch
    """


    environmentConfig?: DataprocCnrmCloudGoogleComV1alpha1DataprocBatchSpecEnvironmentConfig

    labels?: {str:str}

    location?: str

    projectRef?: DataprocCnrmCloudGoogleComV1alpha1DataprocBatchSpecProjectRef

    pysparkBatch?: DataprocCnrmCloudGoogleComV1alpha1DataprocBatchSpecPysparkBatch

    resourceID?: str

    runtimeConfig?: DataprocCnrmCloudGoogleComV1alpha1DataprocBatchSpecRuntimeConfig

    sparkBatch?: DataprocCnrmCloudGoogleComV1alpha1DataprocBatchSpecSparkBatch

    sparkRBatch?: DataprocCnrmCloudGoogleComV1alpha1DataprocBatchSpecSparkRBatch

    sparkSQLBatch?: DataprocCnrmCloudGoogleComV1alpha1DataprocBatchSpecSparkSQLBatch


schema DataprocCnrmCloudGoogleComV1alpha1DataprocBatchSpecEnvironmentConfig:
    r"""
    Optional. Environment configuration for the batch execution.

    Attributes
    ----------
    executionConfig : DataprocCnrmCloudGoogleComV1alpha1DataprocBatchSpecEnvironmentConfigExecutionConfig, default is Undefined, optional
        execution config
    peripheralsConfig : DataprocCnrmCloudGoogleComV1alpha1DataprocBatchSpecEnvironmentConfigPeripheralsConfig, default is Undefined, optional
        peripherals config
    """


    executionConfig?: DataprocCnrmCloudGoogleComV1alpha1DataprocBatchSpecEnvironmentConfigExecutionConfig

    peripheralsConfig?: DataprocCnrmCloudGoogleComV1alpha1DataprocBatchSpecEnvironmentConfigPeripheralsConfig


schema DataprocCnrmCloudGoogleComV1alpha1DataprocBatchSpecEnvironmentConfigExecutionConfig:
    r"""
    Optional. Execution configuration for a workload.

    Attributes
    ----------
    idleTTL : str, default is Undefined, optional
        Optional. Applies to sessions only. The duration to keep the session alive while it's idling. Exceeding this threshold causes the session to terminate. This field cannot be set on a batch workload. Minimum value is 10 minutes; maximum value is 14 days (see JSON representation of [Duration](https://developers.google.com/protocol-buffers/docs/proto3#json)). Defaults to 1 hour if not set. If both `ttl` and `idle_ttl` are specified for an interactive session, the conditions are treated as `OR` conditions: the workload will be terminated when it has been idle for `idle_ttl` or when `ttl` has been exceeded, whichever occurs first.
    kmsKeyRef : DataprocCnrmCloudGoogleComV1alpha1DataprocBatchSpecEnvironmentConfigExecutionConfigKmsKeyRef, default is Undefined, optional
        kms key ref
    networkTags : [str], default is Undefined, optional
        Optional. Tags used for network traffic control.
    networkURI : str, default is Undefined, optional
        Optional. Network URI to connect workload to.
    serviceAccountRef : DataprocCnrmCloudGoogleComV1alpha1DataprocBatchSpecEnvironmentConfigExecutionConfigServiceAccountRef, default is Undefined, optional
        service account ref
    stagingBucketRef : DataprocCnrmCloudGoogleComV1alpha1DataprocBatchSpecEnvironmentConfigExecutionConfigStagingBucketRef, default is Undefined, optional
        staging bucket ref
    subnetworkURI : str, default is Undefined, optional
        Optional. Subnetwork URI to connect workload to.
    ttl : str, default is Undefined, optional
        Optional. The duration after which the workload will be terminated, specified as the JSON representation for [Duration](https://protobuf.dev/programming-guides/proto3/#json). When the workload exceeds this duration, it will be unconditionally terminated without waiting for ongoing work to finish. If `ttl` is not specified for a batch workload, the workload will be allowed to run until it exits naturally (or run forever without exiting). If `ttl` is not specified for an interactive session, it defaults to 24 hours. If `ttl` is not specified for a batch that uses 2.1+ runtime version, it defaults to 4 hours. Minimum value is 10 minutes; maximum value is 14 days. If both `ttl` and `idle_ttl` are specified (for an interactive session), the conditions are treated as `OR` conditions: the workload will be terminated when it has been idle for `idle_ttl` or when `ttl` has been exceeded, whichever occurs first.
    """


    idleTTL?: str

    kmsKeyRef?: DataprocCnrmCloudGoogleComV1alpha1DataprocBatchSpecEnvironmentConfigExecutionConfigKmsKeyRef

    networkTags?: [str]

    networkURI?: str

    serviceAccountRef?: DataprocCnrmCloudGoogleComV1alpha1DataprocBatchSpecEnvironmentConfigExecutionConfigServiceAccountRef

    stagingBucketRef?: DataprocCnrmCloudGoogleComV1alpha1DataprocBatchSpecEnvironmentConfigExecutionConfigStagingBucketRef

    subnetworkURI?: str

    ttl?: str


schema DataprocCnrmCloudGoogleComV1alpha1DataprocBatchSpecEnvironmentConfigExecutionConfigKmsKeyRef:
    r"""
    Optional. The Cloud KMS key to use for encryption.

    Attributes
    ----------
    external : str, default is Undefined, optional
        A reference to an externally managed KMSCryptoKey. Should be in the format `projects/[kms_project_id]/locations/[region]/keyRings/[key_ring_id]/cryptoKeys/[key]`.
    name : str, default is Undefined, optional
        The `name` of a `KMSCryptoKey` resource.
    namespace : str, default is Undefined, optional
        The `namespace` of a `KMSCryptoKey` resource.
    """


    external?: str

    name?: str

    namespace?: str


schema DataprocCnrmCloudGoogleComV1alpha1DataprocBatchSpecEnvironmentConfigExecutionConfigServiceAccountRef:
    r"""
    Optional. Service account that used to execute workload.

    Attributes
    ----------
    external : str, default is Undefined, optional
        The `email` field of an `IAMServiceAccount` resource.
    name : str, default is Undefined, optional
        Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
    namespace : str, default is Undefined, optional
        Namespace of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/
    """


    external?: str

    name?: str

    namespace?: str


schema DataprocCnrmCloudGoogleComV1alpha1DataprocBatchSpecEnvironmentConfigExecutionConfigStagingBucketRef:
    r"""
    Optional. A Cloud Storage bucket used to stage workload dependencies, config files, and store workload output and other ephemeral data, such as Spark history files. If you do not specify a staging bucket, Cloud Dataproc will determine a Cloud Storage location according to the region where your workload is running, and then create and manage project-level, per-location staging and temporary buckets. **This field requires a Cloud Storage bucket name, not a `gs://...` URI to a Cloud Storage bucket.**

    Attributes
    ----------
    external : str, default is Undefined, optional
        A reference to an externally-managed StorageBucket resource.
    name : str, default is Undefined, optional
        The name of a StorageBucket resource.
    namespace : str, default is Undefined, optional
        The namespace of a StorageBucket resource.
    """


    external?: str

    name?: str

    namespace?: str


schema DataprocCnrmCloudGoogleComV1alpha1DataprocBatchSpecEnvironmentConfigPeripheralsConfig:
    r"""
    Optional. Peripherals configuration that workload has access to.

    Attributes
    ----------
    metastoreService : str, default is Undefined, optional
        Optional. Resource name of an existing Dataproc Metastore service.

         Example:

         * `projects/[project_id]/locations/[region]/services/[service_id]`
    sparkHistoryServerConfig : DataprocCnrmCloudGoogleComV1alpha1DataprocBatchSpecEnvironmentConfigPeripheralsConfigSparkHistoryServerConfig, default is Undefined, optional
        spark history server config
    """


    metastoreService?: str

    sparkHistoryServerConfig?: DataprocCnrmCloudGoogleComV1alpha1DataprocBatchSpecEnvironmentConfigPeripheralsConfigSparkHistoryServerConfig


schema DataprocCnrmCloudGoogleComV1alpha1DataprocBatchSpecEnvironmentConfigPeripheralsConfigSparkHistoryServerConfig:
    r"""
    Optional. The Spark History Server configuration for the workload.

    Attributes
    ----------
    dataprocClusterRef : DataprocCnrmCloudGoogleComV1alpha1DataprocBatchSpecEnvironmentConfigPeripheralsConfigSparkHistoryServerConfigDataprocClusterRef, default is Undefined, optional
        dataproc cluster ref
    """


    dataprocClusterRef?: DataprocCnrmCloudGoogleComV1alpha1DataprocBatchSpecEnvironmentConfigPeripheralsConfigSparkHistoryServerConfigDataprocClusterRef


schema DataprocCnrmCloudGoogleComV1alpha1DataprocBatchSpecEnvironmentConfigPeripheralsConfigSparkHistoryServerConfigDataprocClusterRef:
    r"""
    Optional. Resource name of an existing Dataproc Cluster to act as a Spark
     History Server for the workload.

     Example:

     * `projects/[project_id]/regions/[region]/clusters/[cluster_name]`

    Attributes
    ----------
    external : str, default is Undefined, optional
        A reference to an externally managed DataprocCluster resource. Should be in the format "projects/{{projectID}}/regions/{{region}}/clusters/{{clusterName}}".
    name : str, default is Undefined, optional
        The name of a DataprocCluster resource.
    namespace : str, default is Undefined, optional
        The namespace of a DataprocCluster resource.
    """


    external?: str

    name?: str

    namespace?: str


schema DataprocCnrmCloudGoogleComV1alpha1DataprocBatchSpecProjectRef:
    r"""
    Required.

    Attributes
    ----------
    external : str, default is Undefined, optional
        The `projectID` field of a project, when not managed by Config Connector.
    kind : str, default is Undefined, optional
        The kind of the Project resource; optional but must be `Project` if provided.
    name : str, default is Undefined, optional
        The `name` field of a `Project` resource.
    namespace : str, default is Undefined, optional
        The `namespace` field of a `Project` resource.
    """


    external?: str

    kind?: str

    name?: str

    namespace?: str


schema DataprocCnrmCloudGoogleComV1alpha1DataprocBatchSpecPysparkBatch:
    r"""
    Optional. PySpark batch config.

    Attributes
    ----------
    archiveURIs : [str], default is Undefined, optional
        Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: `.jar`, `.tar`, `.tar.gz`, `.tgz`, and `.zip`.
    args : [str], default is Undefined, optional
        Optional. The arguments to pass to the driver. Do not include arguments that can be set as batch properties, such as `--conf`, since a collision can occur that causes an incorrect batch submission.
    fileURIs : [str], default is Undefined, optional
        Optional. HCFS URIs of files to be placed in the working directory of each executor.
    jarFileURIs : [str], default is Undefined, optional
        Optional. HCFS URIs of jar files to add to the classpath of the Spark driver and tasks.
    mainPythonFileURI : str, default is Undefined, optional
        Required. The HCFS URI of the main Python file to use as the Spark driver. Must be a .py file.
    pythonFileURIs : [str], default is Undefined, optional
        Optional. HCFS file URIs of Python files to pass to the PySpark framework. Supported file types: `.py`, `.egg`, and `.zip`.
    """


    archiveURIs?: [str]

    args?: [str]

    fileURIs?: [str]

    jarFileURIs?: [str]

    mainPythonFileURI?: str

    pythonFileURIs?: [str]


schema DataprocCnrmCloudGoogleComV1alpha1DataprocBatchSpecRuntimeConfig:
    r"""
    Optional. Runtime configuration for the batch execution.

    Attributes
    ----------
    autotuningConfig : DataprocCnrmCloudGoogleComV1alpha1DataprocBatchSpecRuntimeConfigAutotuningConfig, default is Undefined, optional
        autotuning config
    cohort : str, default is Undefined, optional
        Optional. Cohort identifier. Identifies families of the workloads having the same shape, e.g. daily ETL jobs.
    containerImage : str, default is Undefined, optional
        Optional. Optional custom container image for the job runtime environment. If not specified, a default container image will be used.
    properties : {str:str}, default is Undefined, optional
        Optional. A mapping of property names to values, which are used to configure workload execution.
    repositoryConfig : DataprocCnrmCloudGoogleComV1alpha1DataprocBatchSpecRuntimeConfigRepositoryConfig, default is Undefined, optional
        repository config
    version : str, default is Undefined, optional
        Optional. Version of the batch runtime.
    """


    autotuningConfig?: DataprocCnrmCloudGoogleComV1alpha1DataprocBatchSpecRuntimeConfigAutotuningConfig

    cohort?: str

    containerImage?: str

    properties?: {str:str}

    repositoryConfig?: DataprocCnrmCloudGoogleComV1alpha1DataprocBatchSpecRuntimeConfigRepositoryConfig

    version?: str


schema DataprocCnrmCloudGoogleComV1alpha1DataprocBatchSpecRuntimeConfigAutotuningConfig:
    r"""
    Optional. Autotuning configuration of the workload.

    Attributes
    ----------
    scenarios : [str], default is Undefined, optional
        Optional. Scenarios for which tunings are applied.
    """


    scenarios?: [str]


schema DataprocCnrmCloudGoogleComV1alpha1DataprocBatchSpecRuntimeConfigRepositoryConfig:
    r"""
    Optional. Dependency repository configuration.

    Attributes
    ----------
    pypiRepositoryConfig : DataprocCnrmCloudGoogleComV1alpha1DataprocBatchSpecRuntimeConfigRepositoryConfigPypiRepositoryConfig, default is Undefined, optional
        pypi repository config
    """


    pypiRepositoryConfig?: DataprocCnrmCloudGoogleComV1alpha1DataprocBatchSpecRuntimeConfigRepositoryConfigPypiRepositoryConfig


schema DataprocCnrmCloudGoogleComV1alpha1DataprocBatchSpecRuntimeConfigRepositoryConfigPypiRepositoryConfig:
    r"""
    Optional. Configuration for PyPi repository.

    Attributes
    ----------
    pypiRepository : str, default is Undefined, optional
        Optional. PyPi repository address
    """


    pypiRepository?: str


schema DataprocCnrmCloudGoogleComV1alpha1DataprocBatchSpecSparkBatch:
    r"""
    Optional. Spark batch config.

    Attributes
    ----------
    archiveURIs : [str], default is Undefined, optional
        Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: `.jar`, `.tar`, `.tar.gz`, `.tgz`, and `.zip`.
    args : [str], default is Undefined, optional
        Optional. The arguments to pass to the driver. Do not include arguments that can be set as batch properties, such as `--conf`, since a collision can occur that causes an incorrect batch submission.
    fileURIs : [str], default is Undefined, optional
        Optional. HCFS URIs of files to be placed in the working directory of each executor.
    jarFileURIs : [str], default is Undefined, optional
        Optional. HCFS URIs of jar files to add to the classpath of the Spark driver and tasks.
    mainClass : str, default is Undefined, optional
        Optional. The name of the driver main class. The jar file that contains the class must be in the classpath or specified in `jar_file_uris`.
    mainJarFileURI : str, default is Undefined, optional
        Optional. The HCFS URI of the jar file that contains the main class.
    """


    archiveURIs?: [str]

    args?: [str]

    fileURIs?: [str]

    jarFileURIs?: [str]

    mainClass?: str

    mainJarFileURI?: str


schema DataprocCnrmCloudGoogleComV1alpha1DataprocBatchSpecSparkRBatch:
    r"""
    Optional. SparkR batch config.

    Attributes
    ----------
    archiveURIs : [str], default is Undefined, optional
        Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: `.jar`, `.tar`, `.tar.gz`, `.tgz`, and `.zip`.
    args : [str], default is Undefined, optional
        Optional. The arguments to pass to the Spark driver. Do not include arguments that can be set as batch properties, such as `--conf`, since a collision can occur that causes an incorrect batch submission.
    fileURIs : [str], default is Undefined, optional
        Optional. HCFS URIs of files to be placed in the working directory of each executor.
    mainRFileURI : str, default is Undefined, optional
        Required. The HCFS URI of the main R file to use as the driver. Must be a `.R` or `.r` file.
    """


    archiveURIs?: [str]

    args?: [str]

    fileURIs?: [str]

    mainRFileURI?: str


schema DataprocCnrmCloudGoogleComV1alpha1DataprocBatchSpecSparkSQLBatch:
    r"""
    Optional. SparkSql batch config.

    Attributes
    ----------
    jarFileURIs : [str], default is Undefined, optional
        Optional. HCFS URIs of jar files to be added to the Spark CLASSPATH.
    queryFileURI : str, default is Undefined, optional
        Required. The HCFS URI of the script that contains Spark SQL queries to execute.
    queryVariables : {str:str}, default is Undefined, optional
        Optional. Mapping of query variable names to values (equivalent to the Spark SQL command: `SET name="value";`).
    """


    jarFileURIs?: [str]

    queryFileURI?: str

    queryVariables?: {str:str}


schema DataprocCnrmCloudGoogleComV1alpha1DataprocBatchStatus:
    r"""
    DataprocBatchStatus defines the config connector machine state of DataprocBatch

    Attributes
    ----------
    conditions : [DataprocCnrmCloudGoogleComV1alpha1DataprocBatchStatusConditionsItems0], default is Undefined, optional
        Conditions represent the latest available observations of the object's current state.
    externalRef : str, default is Undefined, optional
        A unique specifier for the DataprocBatch resource in GCP.
    observedGeneration : int, default is Undefined, optional
        ObservedGeneration is the generation of the resource that was most recently observed by the Config Connector controller. If this is equal to metadata.generation, then that means that the current reported status reflects the most recent desired state of the resource.
    observedState : DataprocCnrmCloudGoogleComV1alpha1DataprocBatchStatusObservedState, default is Undefined, optional
        observed state
    """


    conditions?: [DataprocCnrmCloudGoogleComV1alpha1DataprocBatchStatusConditionsItems0]

    externalRef?: str

    observedGeneration?: int

    observedState?: DataprocCnrmCloudGoogleComV1alpha1DataprocBatchStatusObservedState


schema DataprocCnrmCloudGoogleComV1alpha1DataprocBatchStatusConditionsItems0:
    r"""
    dataproc cnrm cloud google com v1alpha1 dataproc batch status conditions items0

    Attributes
    ----------
    lastTransitionTime : str, default is Undefined, optional
        Last time the condition transitioned from one status to another.
    message : str, default is Undefined, optional
        Human-readable message indicating details about last transition.
    reason : str, default is Undefined, optional
        Unique, one-word, CamelCase reason for the condition's last transition.
    status : str, default is Undefined, optional
        Status is the status of the condition. Can be True, False, Unknown.
    $type : str, default is Undefined, optional
        Type is the type of the condition.
    """


    lastTransitionTime?: str

    message?: str

    reason?: str

    status?: str

    $type?: str


schema DataprocCnrmCloudGoogleComV1alpha1DataprocBatchStatusObservedState:
    r"""
    ObservedState is the state of the resource as most recently observed in GCP.

    Attributes
    ----------
    createTime : str, default is Undefined, optional
        Output only. The time when the batch was created.
    creator : str, default is Undefined, optional
        Output only. The email address of the user who created the batch.
    operation : str, default is Undefined, optional
        Output only. The resource name of the operation associated with this batch.
    runtimeInfo : DataprocCnrmCloudGoogleComV1alpha1DataprocBatchStatusObservedStateRuntimeInfo, default is Undefined, optional
        runtime info
    state : str, default is Undefined, optional
        Output only. The state of the batch.
    stateHistory : [DataprocCnrmCloudGoogleComV1alpha1DataprocBatchStatusObservedStateStateHistoryItems0], default is Undefined, optional
        Output only. Historical state information for the batch.
    stateMessage : str, default is Undefined, optional
        Output only. Batch state details, such as a failure description if the state is `FAILED`.
    stateTime : str, default is Undefined, optional
        Output only. The time when the batch entered a current state.
    uuid : str, default is Undefined, optional
        Output only. A batch UUID (Unique Universal Identifier). The service generates this value when it creates the batch.
    """


    createTime?: str

    creator?: str

    operation?: str

    runtimeInfo?: DataprocCnrmCloudGoogleComV1alpha1DataprocBatchStatusObservedStateRuntimeInfo

    state?: str

    stateHistory?: [DataprocCnrmCloudGoogleComV1alpha1DataprocBatchStatusObservedStateStateHistoryItems0]

    stateMessage?: str

    stateTime?: str

    uuid?: str


schema DataprocCnrmCloudGoogleComV1alpha1DataprocBatchStatusObservedStateRuntimeInfo:
    r"""
    Output only. Runtime information about batch execution.

    Attributes
    ----------
    approximateUsage : DataprocCnrmCloudGoogleComV1alpha1DataprocBatchStatusObservedStateRuntimeInfoApproximateUsage, default is Undefined, optional
        approximate usage
    currentUsage : DataprocCnrmCloudGoogleComV1alpha1DataprocBatchStatusObservedStateRuntimeInfoCurrentUsage, default is Undefined, optional
        current usage
    diagnosticOutputURI : str, default is Undefined, optional
        Output only. A URI pointing to the location of the diagnostics tarball.
    endpoints : {str:str}, default is Undefined, optional
        Output only. Map of remote access endpoints (such as web interfaces and APIs) to their URIs.
    outputURI : str, default is Undefined, optional
        Output only. A URI pointing to the location of the stdout and stderr of the workload.
    """


    approximateUsage?: DataprocCnrmCloudGoogleComV1alpha1DataprocBatchStatusObservedStateRuntimeInfoApproximateUsage

    currentUsage?: DataprocCnrmCloudGoogleComV1alpha1DataprocBatchStatusObservedStateRuntimeInfoCurrentUsage

    diagnosticOutputURI?: str

    endpoints?: {str:str}

    outputURI?: str


schema DataprocCnrmCloudGoogleComV1alpha1DataprocBatchStatusObservedStateRuntimeInfoApproximateUsage:
    r"""
    Output only. Approximate workload resource usage, calculated when
     the workload completes (see [Dataproc Serverless pricing]
     (https://cloud.google.com/dataproc-serverless/pricing)).

     **Note:** This metric calculation may change in the future, for
     example, to capture cumulative workload resource
     consumption during workload execution (see the
     [Dataproc Serverless release notes]
     (https://cloud.google.com/dataproc-serverless/docs/release-notes)
     for announcements, changes, fixes
     and other Dataproc developments).

    Attributes
    ----------
    acceleratorType : str, default is Undefined, optional
        Optional. Accelerator type being used, if any
    milliAcceleratorSeconds : int, default is Undefined, optional
        Optional. Accelerator usage in (`milliAccelerator` x `seconds`) (see [Dataproc Serverless pricing] (https://cloud.google.com/dataproc-serverless/pricing)).
    milliDcuSeconds : int, default is Undefined, optional
        Optional. DCU (Dataproc Compute Units) usage in (`milliDCU` x `seconds`) (see [Dataproc Serverless pricing] (https://cloud.google.com/dataproc-serverless/pricing)).
    shuffleStorageGBSeconds : int, default is Undefined, optional
        Optional. Shuffle storage usage in (`GB` x `seconds`) (see [Dataproc Serverless pricing] (https://cloud.google.com/dataproc-serverless/pricing)).
    """


    acceleratorType?: str

    milliAcceleratorSeconds?: int

    milliDcuSeconds?: int

    shuffleStorageGBSeconds?: int


schema DataprocCnrmCloudGoogleComV1alpha1DataprocBatchStatusObservedStateRuntimeInfoCurrentUsage:
    r"""
    Output only. Snapshot of current workload resource usage.

    Attributes
    ----------
    acceleratorType : str, default is Undefined, optional
        Optional. Accelerator type being used, if any
    milliAccelerator : int, default is Undefined, optional
        Optional. Milli (one-thousandth) accelerator. (see [Dataproc Serverless pricing] (https://cloud.google.com/dataproc-serverless/pricing))
    milliDcu : int, default is Undefined, optional
        Optional. Milli (one-thousandth) Dataproc Compute Units (DCUs) (see [Dataproc Serverless pricing] (https://cloud.google.com/dataproc-serverless/pricing)).
    milliDcuPremium : int, default is Undefined, optional
        Optional. Milli (one-thousandth) Dataproc Compute Units (DCUs) charged at premium tier (see [Dataproc Serverless pricing] (https://cloud.google.com/dataproc-serverless/pricing)).
    shuffleStorageGB : int, default is Undefined, optional
        Optional. Shuffle Storage in gigabytes (GB). (see [Dataproc Serverless pricing] (https://cloud.google.com/dataproc-serverless/pricing))
    shuffleStorageGBPremium : int, default is Undefined, optional
        Optional. Shuffle Storage in gigabytes (GB) charged at premium tier. (see [Dataproc Serverless pricing] (https://cloud.google.com/dataproc-serverless/pricing))
    snapshotTime : str, default is Undefined, optional
        Optional. The timestamp of the usage snapshot.
    """


    acceleratorType?: str

    milliAccelerator?: int

    milliDcu?: int

    milliDcuPremium?: int

    shuffleStorageGB?: int

    shuffleStorageGBPremium?: int

    snapshotTime?: str


schema DataprocCnrmCloudGoogleComV1alpha1DataprocBatchStatusObservedStateStateHistoryItems0:
    r"""
    dataproc cnrm cloud google com v1alpha1 dataproc batch status observed state state history items0

    Attributes
    ----------
    state : str, default is Undefined, optional
        Output only. The state of the batch at this point in history.
    stateMessage : str, default is Undefined, optional
        Output only. Details about the state at this point in history.
    stateStartTime : str, default is Undefined, optional
        Output only. The time when the batch entered the historical state.
    """


    state?: str

    stateMessage?: str

    stateStartTime?: str


